{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605380a2-a3d9-4ba1-911e-618e8739d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.8/183.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.24.0 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e6631-c078-4bd7-8b51-6f1b702481d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6243f9a6-212b-4e97-95c9-472b2a1e8d07",
   "metadata": {},
   "source": [
    "### Part 1: Upderstanding Weight Initialization\n",
    "\n",
    "Q1 Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully?\n",
    "\n",
    "Answer--> Weight initialization is crucial in artificial neural networks because it significantly influences the learning process and the overall performance of the network. Properly initializing the weights helps in overcoming common issues such as vanishing or exploding gradients during training.\n",
    "\n",
    "Careful weight initialization techniques, such as Xavier/Glorot or He initialization, help to strike a balance by setting the initial weights in a way that promotes stable and efficient learning. This ensures that the network converges faster and performs better on the given task, contributing to the overall success of the neural network training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ba05b-ab90-4583-bacc-f867bc386081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6df9a824-a3b4-460e-9d16-e0da0ac9bc94",
   "metadata": {},
   "source": [
    "\n",
    "Q2 Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "\n",
    "Answer--> Challenges with improper weight initialization include:\n",
    "\n",
    "1. **Vanishing/Exploding Gradients**: Incorrectly initialized weights can cause gradients to become extremely small (vanishing) or large (exploding), hampering convergence or causing divergence during training.\n",
    "\n",
    "2. **Slow Convergence**: Inadequate initialization might lead to slower convergence rates, increasing the time and resources required for training.\n",
    "\n",
    "3. **Stuck in Local Minima**: Poor initialization may cause the network to get stuck in local minima, preventing it from finding the optimal solution.\n",
    "\n",
    "4. **Unstable Training**: Unstable weights can lead to erratic training behavior, where the model's performance fluctuates unpredictably during training.\n",
    "\n",
    "5. **Sensitivity to Architecture Changes**: Inadequate initialization can make the network sensitive to changes in architecture or hyperparameters, making it harder to generalize across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d11a03-e66f-443c-b344-4df4f97a66c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e186bc-1675-4201-8417-97e9dfec6a6a",
   "metadata": {},
   "source": [
    "Q3 Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "Answer--> Variance refers to the measure of how much the values in a dataset differ from the mean. In the context of weight initialization in neural networks, variance becomes crucial because it influences how information propagates through the network during forward and backward passes.\n",
    "\n",
    "Considering the variance of weights during initialization matters because it directly impacts how information propagates through the network. Proper variance ensures stable learning by preventing issues like vanishing or exploding gradients, leading to faster convergence and improved network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211f24d-c9d3-4982-8aee-00bdc9f73b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629c0fcd-bfdc-4c01-9698-3f7f2a0d8c8f",
   "metadata": {},
   "source": [
    "Part 2: Weight Initialization Techpique\n",
    "\n",
    "Q4 Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "\n",
    "Answer--> Zero initialization involves setting all weights or parameters in a neural network to zero initially. It's straightforward and easy to implement but comes with limitations:\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "1. **Symmetry Issues:** Initializing all weights to zero causes symmetry among neurons. During training, all neurons will compute the same output, leading to the symmetric behavior of neurons throughout the network. This symmetry persists through subsequent iterations, hindering the learning process as the model fails to learn diverse representations.\n",
    "\n",
    "2. **Vanishing Gradient:** With zero initialization, neurons in deeper layers may suffer from vanishing gradients. This occurs because all weights are the same, causing identical updates during backpropagation, resulting in slow or no learning in deeper layers.\n",
    "\n",
    "**When it can be appropriate to use:**\n",
    "\n",
    "1. **Bias Initialization:** Zero initialization can be useful for initializing biases in some cases, where symmetry doesn't pose a problem as biases are meant to introduce asymmetry in the neuron activations.\n",
    "\n",
    "2. **Certain Activation Functions:** Some activation functions, like ReLU (Rectified Linear Unit), can tolerate zero initialization for certain weights without facing symmetry issues. This applies specifically to biases and certain initializations within the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de111b70-832a-42e8-8ccb-609adda22f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f944c974-b600-4170-8caf-407bbbbba8ea",
   "metadata": {},
   "source": [
    "Q5 Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients?\n",
    "\n",
    "Answer--> Random initialization involves setting the weights of a neural network to random values, typically drawn from a distribution like uniform or normal distribution.\n",
    "\n",
    "By adjusting the scale of random initialization based on the characteristics of the activation functions and the architecture of the neural network, it's possible to control the variance of weights and activations. This helps in mitigating issues like saturation, vanishing, or exploding gradients, ensuring stable and efficient learning during the training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d377536-4558-4521-81d0-324eaad54f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "784c888b-7ad8-484b-8c9d-7b9836ccaf51",
   "metadata": {},
   "source": [
    "Q6 Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it.\n",
    "\n",
    "Answer--> Xavier/Glorot initialization is a technique used to initialize the weights in a neural network based on the size of the input and output layers. It addresses the challenges of improper weight initialization by controlling the variance of weights, aiming to ensure stable and efficient learning.\n",
    "\n",
    "The key benefits of Xavier/Glorot initialization are:\n",
    "\n",
    "1. **Preventing Vanishing/Exploding Gradients:** By carefully adjusting the variance of weights, this initialization method helps in preventing gradients from becoming too small (vanishing) or too large (exploding), which can hinder or destabilize the learning process.\n",
    "\n",
    "2. **Stable Learning:** By maintaining a consistent variance of activations, Xavier/Glorot initialization promotes stable learning across different layers of the neural network.\n",
    "\n",
    "3. **Applicability to Various Activation Functions:** It's designed to work well with various activation functions like tanh or sigmoid, ensuring that the initialization is suitable for different types of nonlinearities commonly used in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bc5b1-d525-44c0-9f9f-c81d85478616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a15cea0-ef45-4710-aba1-3a3ad4612c20",
   "metadata": {},
   "source": [
    "Q7 Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n",
    "\n",
    "Answer--> He initialization, named after its creator Kaiming He, is a weight initialization technique that differs from Xavier/Glorot initialization in the way it handles the variance of weights. It is particularly suited for networks that use rectified linear units (ReLU) or its variants as activation functions.\n",
    "\n",
    "The key difference between He initialization and Xavier initialization lies in the scaling factor used to set the variance of weights. While Xavier initialization considers both the number of input and output neurons, He initialization only accounts for the number of input neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1970ad-630b-4f2a-b7d6-076a45fb8024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6ee7d2a-0711-4c03-a5bd-8fd4b180ced9",
   "metadata": {},
   "source": [
    "Part 3: Applying Weight Ipitialization\n",
    "\n",
    "Q8 Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of your choice. Train the model on a suitable dataset and compare the performance of the initialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909b012b-9763-42cd-ace6-e13d50fb1361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Zero Initialized model...\n",
      "Training Random Initialized model...\n",
      "Training Xavier Initialized model...\n",
      "Training He Initialized model...\n",
      "Model: Zero Initialized, Test Accuracy: 11.3500\n",
      "Model: Random Initialized, Test Accuracy: 97.7000\n",
      "Model: Xavier Initialized, Test Accuracy: 97.8300\n",
      "Model: He Initialized, Test Accuracy: 97.8500\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import initializers\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28 * 28)) / 255.0\n",
    "x_test = x_test.reshape((-1, 28 * 28)) / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Define function to create models with different weight initializations\n",
    "def create_model(initializer):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(784,), activation='relu', kernel_initializer=initializer))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer=initializer))\n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer=initializer))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize models with different initializers\n",
    "zero_initialized_model = create_model(initializers.Zeros())\n",
    "random_initialized_model = create_model(initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42))\n",
    "xavier_initialized_model = create_model(initializers.glorot_normal())\n",
    "he_initialized_model = create_model(initializers.he_normal())\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Zero Initialized': zero_initialized_model,\n",
    "    'Random Initialized': random_initialized_model,\n",
    "    'Xavier Initialized': xavier_initialized_model,\n",
    "    'He Initialized': he_initialized_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    history = model.fit(x_train, y_train, epochs=10,\n",
    "                        batch_size=128, \n",
    "                        validation_data=(x_test, y_test), verbose=0)\n",
    "    results[name] = history\n",
    "\n",
    "# Compare performances\n",
    "for name, history in results.items():\n",
    "    print(f\"Model: {name}, Test Accuracy: {np.max(history.history['val_accuracy'])*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c1bcf-8af9-453c-aacd-b7896684ce61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c69f54e-98e2-4138-aaf3-a343d0162979",
   "metadata": {},
   "source": [
    "Q9 Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
    "\n",
    "Answer--> Choosing the appropriate weight initialization technique involves understanding the network architecture, the nature of the task, the activation functions used, and balancing tradeoffs between computational cost, convergence speed, and generalization performance. Experimentation and empirical validation are crucial to identify the most suitable initialization method for a specific scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba639faf-964f-4750-b890-065e45b5acc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67b83c-e08f-440f-9b61-7d766d18ee47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
