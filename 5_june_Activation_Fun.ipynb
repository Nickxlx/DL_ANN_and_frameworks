{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6287b24-cbd7-47d7-90df-906b109a283b",
   "metadata": {},
   "source": [
    "Q1. What is an activation function in the context of artificial neural networks?\n",
    "\n",
    "Answer--> Activation function is a critical component of ANN that adds non-linearity in the output of a layer or neuron. It determines weather a neuron should be activated or not based on the submission of weighted input along with biase reached a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d6d02-964b-4bbf-aa03-98a654051f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2342738-5a28-417c-9323-bb6c1775ef8a",
   "metadata": {},
   "source": [
    "Q2. What are some common types of activation functions used in neural networks?\n",
    "\n",
    "Answer--> Here are some common activation functions include:\n",
    "\n",
    "1 ReLU(x): It returns the x for +ve and zero , for -ve give 0.\n",
    "\n",
    "ADVANTAGE\n",
    "- its simple to understand and solve the problem of vanishing gradient \n",
    "\n",
    "DISADVANTAGE\n",
    "\n",
    "- raise the proble of dying neurons  \n",
    "\n",
    "2 leaky ReLU(x): It a variation of ReLU function in which we fix a learing rate of apha=0.1 with the output that returns the x for +ve and zero otherwise for -ve give x*alpha.\n",
    "\n",
    "ADVANTAGE\n",
    "- previent from the dying neuron  \n",
    "\n",
    "3 Sigmoid : It squased the output of the function between 0 and 1.\n",
    "\n",
    "ADVANTAGE\n",
    "- smothing gradient vanishing.  \n",
    "\n",
    "DISADVANTAGE\n",
    "- vanishing gradient.  \n",
    "\n",
    "4 Hyperbolic Tangent (Tanh) Function :  It is simmilar to sigmoid sunction but squased the output of the function between -1 and 1. It can also use for classication problems.\n",
    "\n",
    "ADVANTAGE\n",
    "- here we can take large learning steps so conversance becomes fast \n",
    " \n",
    "DISADVANTAGE\n",
    "- Suffers from vanishing gradient.\n",
    "\n",
    "5 Softmax Function: Use in the output layer for binary or multiclass both problems that gives the vector of probablities. \n",
    "\n",
    "ADVANTAGE\n",
    "- return the probability for the highest calss. \n",
    "- output ranges between o to 1 for each class.\n",
    "\n",
    "DISADVANTAGE\n",
    "\n",
    "- vanishing gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903a471-4fbd-48a1-8f3b-581e87bd0454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d7a0e2-c795-4d66-ae2d-917712fc7a45",
   "metadata": {},
   "source": [
    "Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "\n",
    "Answer --> Activation functions play a crucial role in neural networks, impacting the training process and the network's performance in several ways:\n",
    "\n",
    "1. **Non-linearity**: Activation functions introduce non-linearities, enabling neural networks to learn and model complex relationships within data. Without non-linear activation functions, the network would only be able to learn linear transformations, severely limiting its expressive power.\n",
    "\n",
    "2. **Gradient Flow**: The choice of activation function influences how gradients flow through the network during backpropagation. Some activation functions (like ReLU) alleviate vanishing gradient problems by avoiding saturation in certain regions of the function, leading to more stable and faster convergence during training.\n",
    "\n",
    "3. **Output Range**: The range of values produced by an activation function affects the scale of activations in a layer. Functions like sigmoid and tanh squish inputs to a specific range (0 to 1 and -1 to 1, respectively), while ReLU produces unbounded positive values. This can impact the stability of training and the dynamics of the network.\n",
    "\n",
    "4. **Robustness to Input Changes**: Some activation functions, like Leaky ReLU, are more robust to changes in input data, ensuring that even small changes in input can lead to changes in the output, which might be advantageous in certain scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52912d0-e212-4159-8c2e-12f5e312cb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed40c25e-a2e8-4075-bee4-a8b9eeadbdf0",
   "metadata": {},
   "source": [
    "Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "\n",
    "Answer--> Sigmoid : This function squashes the input values between 0 and 1. It's useful in binary classification tasks \n",
    "  \n",
    "        σ(x)= 1/1+e^−x\n",
    "\n",
    "ADVANTAGE\n",
    "- smothing gradient vanishing.  \n",
    "\n",
    "DISADVANTAGE\n",
    "- vanishing gradient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb2ca4-7133-47dc-a5db-ed637b4a846a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b83e0f6-de90-4dff-abb7-e89353bdec39",
   "metadata": {},
   "source": [
    "Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "\n",
    "Answer--> \n",
    "The Rectified Linear Unit (ReLU) and the Sigmoid activation functions differ significantly in their behavior and mathematical properties.\n",
    "\n",
    "1 ReLU(x): It returns the x for +ve and zero , for -ve give 0.\n",
    "\n",
    "Sigmoid : This function squashes the input values between 0 and 1. It's useful in binary classification tasks \n",
    "\n",
    "        σ(x)= 1/1+e^−x\n",
    "\n",
    "ReLU is a piecewise linear function that is computationally efficient and helps mitigate the vanishing gradient problem, while the sigmoid function is smooth, interpretable as probabilities, but can suffer from vanishing gradients in deep networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0928a-4993-4629-8752-1f84da943883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0da824c-9dd0-449d-9eff-146b79ba2b32",
   "metadata": {},
   "source": [
    "Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "\n",
    "Answer--> Using the Rectified Linear Unit (ReLU) activation function over the Sigmoid function offers several advantages:\n",
    "\n",
    "1. **Avoiding Vanishing Gradient**:\n",
    "   - ReLU addresses the vanishing gradient problem more effectively than sigmoid. It doesn't saturate for positive inputs, allowing for faster convergence during training by maintaining non-zero gradients for positive values.\n",
    "\n",
    "2. **Computational Efficiency**:\n",
    "   - ReLU is computationally efficient due to its simplicity. It involves straightforward operations like thresholding negative values to zero.\n",
    "\n",
    "3. **Sparsity in Activation**:\n",
    "   - ReLU introduces sparsity by zeroing out negative inputs. This can reduce computational load by having fewer active neurons, thereby reducing redundancy in the network.\n",
    "\n",
    "4. **Ease of Optimization**:\n",
    "   - The piecewise linear nature of ReLU makes it amenable to optimization with gradient-based methods, enabling faster training of deep neural networks compared to sigmoid, especially in deeper architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0432a-0ce9-4e9d-a45f-4ca19ddca140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6038d3d-e9c8-4c3b-af8b-28df96620a60",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "\n",
    "Answer--> Leaky ReLU is a variation of the Rectified Linear Unit (ReLU) activation function that introduces a small slope for negative inputs, unlike the standard ReLU where negative values are completely zeroed out.\n",
    "\n",
    "In Leaky ReLU:\n",
    "- It keeps the standard ReLU (output = 0 for x < 0) for positive input values.\n",
    "- For negative inputs (x < 0), it introduces a small gradient instead of setting the output to zero. The formula can be represented as:\n",
    "\n",
    "   f(x) = max(alpha x, x) \n",
    "     \n",
    "  - Here, ( alpha ) is a small positive slope, usually a very small value like 0.01 or 0.1.\n",
    "\n",
    "### Addressing the Vanishing Gradient Problem:\n",
    "The vanishing gradient problem occurs when gradients become extremely small during backpropagation, slowing down or hindering the learning process in deep neural networks. Leaky ReLU addresses this issue by providing a non-zero gradient for negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195fbcb-e3bb-4a86-9a0b-ac1f0948fa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf3d464-df63-4ece-adc9-d1c396dc8ee6",
   "metadata": {},
   "source": [
    "Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "\n",
    "Answer--> The softmax activation function is primarily used in the output layer of a neural network, especially in multi-class classification problems. Its primary purpose is to normalize the output of a network into a probability distribution across multiple classes, ensuring that the output values sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc3602-34f6-41dd-a405-e4a5fe793e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49be6560-c9c6-41ac-adc5-0047fa817871",
   "metadata": {},
   "source": [
    "Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
    "\n",
    "Answer--> Hyperbolic Tangent (Tanh) Function :  It is simmilar to sigmoid sunction but squased the output of the function between -1 and 1. It can also use for classication problems.\n",
    "\n",
    "## Comparison with Sigmoid:\n",
    "- Output Range: Sigmoid ranges between 0 and 1, while tanh ranges between -1 and 1.\n",
    "- Zero-Centered: Sigmoid function's output is not zero-centered (it ranges from 0 to 1), whereas tanh is centered at 0, making optimization more convenient.\n",
    "- Symmetry: Tanh function is symmetric around the origin (0,0), unlike the sigmoid function which is asymmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc171f4-a194-444c-9c49-ad50b8980f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
